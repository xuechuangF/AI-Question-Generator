import os
import json
import asyncio
import aiohttp
from typing import List, Dict, Any, Optional, Tuple
import PyPDF2
import docx
from dataclasses import dataclass, asdict
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
from tqdm import tqdm
import nest_asyncio
import time

# åœ¨Colabä¸­å¯ç”¨åµŒå¥—çš„äº‹ä»¶å¾ªç¯
nest_asyncio.apply()

@dataclass
class KnowledgePoint:
    """çŸ¥è¯†ç‚¹æ•°æ®ç»“æ„"""
    id: int
    title: str
    summary: str
    context_ref: str = ""
    key_formulas: List[str] = None
    key_terms: List[str] = None
    difficulty_level: str = "åŸºç¡€"  # æ–°å¢ï¼šåŸºç¡€/è¿›é˜¶/é«˜çº§
    knowledge_type: str = "æ¦‚å¿µå®šä¹‰"  # æ–°å¢ï¼šçŸ¥è¯†ç‚¹ç±»å‹

    def __post_init__(self):
        if self.key_formulas is None:
            self.key_formulas = []
        if self.key_terms is None:
            self.key_terms = []

@dataclass
class Question:
    """é¢˜ç›®æ•°æ®ç»“æ„"""
    id: int
    knowledge_point_id: int
    question: str
    options: Dict[str, str]
    correct_answer: str
    explanation: str
    difficulty: str = "medium"
    question_type: str = "åŸºç¡€ç†è§£"  
    related_knowledge_points: List[int] = None  

    def __post_init__(self):
        if self.related_knowledge_points is None:
            self.related_knowledge_points = [self.knowledge_point_id]

class DocumentParser:
    """æ–‡æ¡£è§£æå™¨ï¼Œæ”¯æŒPDFå’ŒWord"""

    @staticmethod
    def extract_text_from_pdf(file_path: str) -> str:
        """ä»PDFä¸­æå–æ–‡æœ¬"""
        text = ""
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text += page.extract_text() + "\n"
        except Exception as e:
            print(f"PDFè§£æé”™è¯¯: {e}")
        return text

    @staticmethod
    def extract_text_from_docx(file_path: str) -> str:
        """ä»Wordæ–‡æ¡£ä¸­æå–æ–‡æœ¬"""
        text = ""
        try:
            doc = docx.Document(file_path)
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"

            # æå–è¡¨æ ¼ä¸­çš„æ–‡æœ¬
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        text += cell.text + "\t"
                    text += "\n"
        except Exception as e:
            print(f"Wordæ–‡æ¡£è§£æé”™è¯¯: {e}")
        return text

    @staticmethod
    def parse_document(file_path: str) -> str:
        """æ ¹æ®æ–‡ä»¶ç±»å‹è§£ææ–‡æ¡£"""
        file_path_lower = file_path.lower()
        
        if file_path_lower.endswith('.pdf'):
            return DocumentParser.extract_text_from_pdf(file_path)
        elif file_path_lower.endswith(('.docx', '.doc')):
            return DocumentParser.extract_text_from_docx(file_path)
        elif file_path_lower.endswith(('.txt', '.md', '.markdown')):
            # æ·»åŠ å¯¹txtå’Œmarkdownæ–‡ä»¶çš„æ”¯æŒ
            try:
                with open(file_path, 'r', encoding='utf-8') as file:
                    return file.read()
            except UnicodeDecodeError:
                # å¦‚æœUTF-8è§£ç å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç 
                with open(file_path, 'r', encoding='gbk') as file:
                    return file.read()
        else:
            raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ã€‚è¯·ä½¿ç”¨PDFã€Wordã€TXTæˆ–Markdownæ–‡æ¡£ã€‚")

class TextChunker:
    """æ–‡æœ¬åˆ†å—å™¨"""

    def __init__(self, quality_level: str = "ä¸­ç­‰"):
        # æ ¹æ®è´¨é‡æ¡£ä½è®¾ç½®å‚æ•° - æ›´åŠ ç²¾ç»†çš„åŒºåˆ†
        quality_configs = {
            "ç®€çº¦": {"max_chunk_size": 8000, "overlap_size": 200, "min_chunk_size": 3000},
            "ä¸­ç­‰": {"max_chunk_size": 4000, "overlap_size": 500, "min_chunk_size": 1500},
            "è¾ƒç»†è‡´": {"max_chunk_size": 2500, "overlap_size": 600, "min_chunk_size": 1000},
            "ç»†è‡´": {"max_chunk_size": 1800, "overlap_size": 700, "min_chunk_size": 800},
            "ç²¾ç»†": {"max_chunk_size": 1200, "overlap_size": 800, "min_chunk_size": 600},
        }

        config = quality_configs.get(quality_level, quality_configs["ä¸­ç­‰"])
        self.max_chunk_size = config["max_chunk_size"]
        self.overlap_size = config["overlap_size"]
        self.min_chunk_size = config["min_chunk_size"]  # æ–°å¢ï¼šæœ€å°åˆ†å—å¤§å°
        self.quality_level = quality_level

    def chunk_text(self, text: str) -> List[Tuple[str, Dict[str, Any]]]:
        """
        å°†æ–‡æœ¬åˆ†å—ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
        è¿”å›: [(chunk_text, metadata), ...]
        """
        # æ ¹æ®è´¨é‡æ¡£ä½é€‰æ‹©ä¸åŒçš„åˆ†å—ç­–ç•¥
        if self.quality_level in ["ç®€çº¦", "ä¸­ç­‰"]:
            return self._chunk_by_paragraphs(text)
        else:
            return self._chunk_by_sentences(text)  

    def _chunk_by_paragraphs(self, text: str) -> List[Tuple[str, Dict[str, Any]]]:
        """æŒ‰æ®µè½åˆ†å—ï¼ˆé€‚ç”¨äºç®€çº¦å’Œä¸­ç­‰æ¡£ä½ï¼‰"""
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        chunk_index = 0

        for i, para in enumerate(paragraphs):
            if len(current_chunk) + len(para) < self.max_chunk_size:
                current_chunk += para + "\n\n"
            else:
                if current_chunk and len(current_chunk) >= self.min_chunk_size:
                    chunks.append((
                        current_chunk.strip(),
                        {
                            "chunk_index": chunk_index,
                            "chunk_type": "paragraph_based",
                            "start_paragraph": i - len(current_chunk.split('\n\n')) + 1,
                            "end_paragraph": i - 1
                        }
                    ))
                    chunk_index += 1

                overlap_text = self._get_overlap_text(current_chunk)
                current_chunk = overlap_text + para + "\n\n"

        if current_chunk and len(current_chunk) >= self.min_chunk_size:
            chunks.append((
                current_chunk.strip(),
                {
                    "chunk_index": chunk_index,
                    "chunk_type": "paragraph_based",
                    "start_paragraph": len(paragraphs) - len(current_chunk.split('\n\n')),
                    "end_paragraph": len(paragraphs) - 1
                }
            ))

        return chunks

    def _chunk_by_sentences(self, text: str) -> List[Tuple[str, Dict[str, Any]]]:
        """æŒ‰å¥å­åˆ†å—ï¼ˆé€‚ç”¨äºè¾ƒç»†è‡´ã€ç»†è‡´ã€ç²¾ç»†æ¡£ä½ï¼‰"""
        import re
        # æ›´ç²¾ç»†çš„å¥å­åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]

        chunks = []
        current_chunk = ""
        chunk_index = 0

        for i, sentence in enumerate(sentences):
            if len(current_chunk) + len(sentence) < self.max_chunk_size:
                current_chunk += sentence + "ã€‚"
            else:
                if current_chunk and len(current_chunk) >= self.min_chunk_size:
                    chunks.append((
                        current_chunk.strip(),
                        {
                            "chunk_index": chunk_index,
                            "chunk_type": "sentence_based",
                            "start_sentence": i - current_chunk.count('ã€‚') + 1,
                            "end_sentence": i - 1,
                            "granularity": "fine"
                        }
                    ))
                    chunk_index += 1

                # ä¿ç•™æ›´å¤šé‡å å†…å®¹ä»¥ä¿æŒè¯­ä¹‰è¿è´¯æ€§
                overlap_sentences = current_chunk.split('ã€‚')[-3:]  # ä¿ç•™æœ€å3å¥
                current_chunk = 'ã€‚'.join(overlap_sentences) + "ã€‚" + sentence + "ã€‚"

        if current_chunk and len(current_chunk) >= self.min_chunk_size:
            chunks.append((
                current_chunk.strip(),
                {
                    "chunk_index": chunk_index,
                    "chunk_type": "sentence_based",
                    "start_sentence": len(sentences) - current_chunk.count('ã€‚'),
                    "end_sentence": len(sentences) - 1,
                    "granularity": "fine"
                }
            ))

        return chunks

    def _get_overlap_text(self, text: str) -> str:
        """è·å–é‡å æ–‡æœ¬"""
        if len(text) <= self.overlap_size:
            return text

        # ä»åå¾€å‰æ‰¾ï¼Œä¿æŒå®Œæ•´çš„å¥å­
        overlap_start = max(0, len(text) - self.overlap_size)
        # æ‰¾åˆ°å¥å­å¼€å§‹ä½ç½®
        while overlap_start > 0 and text[overlap_start] not in '.ã€‚!ï¼?ï¼Ÿ':
            overlap_start -= 1

        return text[overlap_start:].lstrip()

class LLMClient:
    """å¤§æ¨¡å‹APIå®¢æˆ·ç«¯"""

    def __init__(self, api_key: str, base_url: str = "https://api.deepseek.com"):
        self.api_key = api_key
        self.base_url = base_url
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

    async def call_api(self, prompt: str, max_tokens: int = 2000) -> str:
      """å¼‚æ­¥è°ƒç”¨API"""
      async with aiohttp.ClientSession() as session:
          data = {
              "model": "deepseek-chat",
              "messages": [{"role": "user", "content": prompt}],
              "max_tokens": max_tokens,
              "temperature": 0.7
          }

          try:

              async with session.post(
                  f"{self.base_url}/v1/chat/completions",
                  headers=self.headers,
                  json=data
              ) as response:
                  if response.status != 200:
                      error_text = await response.text()
                      print(f"APIé”™è¯¯ (çŠ¶æ€ç  {response.status}): {error_text}")
                      return ""

                  result = await response.json()

                  # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
                  if 'error' in result:
                      print(f"APIè¿”å›é”™è¯¯: {result['error']}")
                      return ""

                  # æ£€æŸ¥æ˜¯å¦æœ‰æ­£ç¡®çš„å“åº”æ ¼å¼
                  if 'choices' not in result or len(result['choices']) == 0:
                      print(f"APIå“åº”æ ¼å¼é”™è¯¯: {result}")
                      return ""

                  return result['choices'][0]['message']['content']
          except Exception as e:
              print(f"APIè°ƒç”¨é”™è¯¯: {e}")
              import traceback
              traceback.print_exc()
              return ""

class KnowledgeExtractor:
    """çŸ¥è¯†ç‚¹æå–å™¨"""

    def __init__(self, llm_client: LLMClient, quality_level: str = "ä¸­ç­‰"):
        self.llm_client = llm_client
        self.quality_level = quality_level

        # æ ¹æ®è´¨é‡æ¡£ä½è®¾ç½®ä¸åŒçš„æå–ç­–ç•¥
        self.extraction_strategies = {
            "ç®€çº¦": {
                "focus": "åªè¯†åˆ«æœ€æ ¸å¿ƒã€æœ€é‡è¦çš„ä¸»è¦æ¦‚å¿µå’ŒåŸç†",
                "detail_level": "å¿½ç•¥ç»†èŠ‚å’Œä¾‹å­ï¼Œä¸“æ³¨äºæ ¸å¿ƒè¦ç‚¹",
                "knowledge_types": ["æ¦‚å¿µå®šä¹‰", "åŸç†æ–¹æ³•"],
                "max_kp_per_chunk": 3
            },
            "ä¸­ç­‰": {
                "focus": "è¯†åˆ«é‡è¦çŸ¥è¯†ç‚¹ï¼ŒåŒ…å«å…³é”®æ¦‚å¿µã€åŸç†å’Œæ–¹æ³•",
                "detail_level": "åŒ…å«å¿…è¦çš„ç»†èŠ‚å’Œé‡è¦ä¾‹å­",
                "knowledge_types": ["æ¦‚å¿µå®šä¹‰", "åŸç†æ–¹æ³•", "å…¬å¼è®¡ç®—", "å®ä¾‹åº”ç”¨"],
                "max_kp_per_chunk": 5
            },
            "è¾ƒç»†è‡´": {
                "focus": "è¯¦ç»†è¯†åˆ«çŸ¥è¯†ç‚¹ï¼ŒåŒ…å«é‡è¦ç»†èŠ‚ã€ä¾‹å­å’Œæ³¨æ„äº‹é¡¹",
                "detail_level": "æ·±å…¥æŒ–æ˜éšå«ä¿¡æ¯å’Œé€»è¾‘å…³ç³»",
                "knowledge_types": ["æ¦‚å¿µå®šä¹‰", "åŸç†æ–¹æ³•", "å…¬å¼è®¡ç®—", "å®ä¾‹åº”ç”¨", "æ³¨æ„äº‹é¡¹"],
                "max_kp_per_chunk": 7
            },
            "ç»†è‡´": {
                "focus": "å…¨é¢è¯†åˆ«çŸ¥è¯†ç‚¹ï¼ŒåŒ…å«æ‰€æœ‰é‡è¦ä¿¡æ¯ã€ç»†èŠ‚ã€ä¾‹å­ã€æ³¨æ„äº‹é¡¹",
                "detail_level": "è¯†åˆ«å‰ææ¡ä»¶ã€é™åˆ¶æ¡ä»¶ã€åº”ç”¨åœºæ™¯å’Œå¯¹æ¯”å…³ç³»",
                "knowledge_types": ["æ¦‚å¿µå®šä¹‰", "åŸç†æ–¹æ³•", "å…¬å¼è®¡ç®—", "å®ä¾‹åº”ç”¨", "æ³¨æ„äº‹é¡¹", "æ¡ä»¶é™åˆ¶", "å¯¹æ¯”åˆ†æ"],
                "max_kp_per_chunk": 10
            },
            "ç²¾ç»†": {
                "focus": "æå…¶è¯¦ç»†åœ°è¯†åˆ«æ‰€æœ‰å¯èƒ½çš„çŸ¥è¯†ç‚¹ï¼ŒåŒ…å«æ¯ä¸ªæ¦‚å¿µã€ç»†èŠ‚ã€ä¾‹å­ã€æ³¨æ„äº‹é¡¹ã€å‰ææ¡ä»¶ç­‰",
                "detail_level": "æå–æ¯ä¸ªé‡è¦çš„ç»†èŠ‚ã€è¡¥å……è¯´æ˜ã€éšå«å‡è®¾å’Œè¾¹ç•Œæ¡ä»¶",
                "knowledge_types": ["æ¦‚å¿µå®šä¹‰", "åŸç†æ–¹æ³•", "å…¬å¼è®¡ç®—", "å®ä¾‹åº”ç”¨", "æ³¨æ„äº‹é¡¹", "æ¡ä»¶é™åˆ¶", "å¯¹æ¯”åˆ†æ", "èƒŒæ™¯ä¿¡æ¯", "æ‰©å±•çŸ¥è¯†"],
                "max_kp_per_chunk": 15
            }
        }

        self.strategy = self.extraction_strategies.get(quality_level, self.extraction_strategies["ä¸­ç­‰"])

        self.extraction_prompt_template = """ä½ æ˜¯ä¸€ä¸ªé«˜çº§å­¦ä¹ è¾…åŠ©AIï¼Œä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·æä¾›çš„å­¦ä¹ èµ„æ–™ï¼Œå°†å…¶åˆ†è§£æˆæ ¸å¿ƒçŸ¥è¯†ç‚¹å¹¶ç»“æ„åŒ–è¾“å‡ºã€‚

**è´¨é‡æ¡£ä½ï¼š** {quality_level}
**æå–ç­–ç•¥ï¼š** {extraction_focus}
**ç»†èŠ‚è¦æ±‚ï¼š** {detail_level}
**é¢„æœŸçŸ¥è¯†ç‚¹ç±»å‹ï¼š** {knowledge_types}

**æ³¨æ„ï¼š** ä½ æ­£åœ¨å¤„ç†æ–‡æ¡£çš„ç¬¬ {chunk_index} éƒ¨åˆ†ã€‚

**è¾“å…¥ï¼š**
{chunk_text}

**ä½ çš„ä»»åŠ¡ï¼š**
1. **æ·±å…¥ç†è§£å†…å®¹ï¼š** ä»”ç»†é˜…è¯»å¹¶ç†è§£è¾“å…¥æ–‡æ¡£çš„æ ¸å¿ƒä¸»é¢˜ã€é€»è¾‘ç»“æ„å’Œå…³é”®ä¿¡æ¯ã€‚

2. **è¯†åˆ«çŸ¥è¯†ç‚¹ï¼š** æ ¹æ®å½“å‰è´¨é‡æ¡£ä½è¦æ±‚ï¼Œå°†æ–‡æ¡£å†…å®¹åˆ†è§£æˆç‹¬ç«‹ã€è‡ªåŒ…å«çš„æ ¸å¿ƒçŸ¥è¯†ç‚¹å•å…ƒï¼š
   {specific_instructions}

3. **ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹åˆ›å»ºç»“æ„åŒ–æ¡ç›®ï¼š**
   - `id`: å”¯ä¸€æ•°å­—åºå· (ä»1å¼€å§‹)
   - `title`: ç®€æ´ã€å‡†ç¡®çš„æ ‡é¢˜ (ä¸è¶…è¿‡15å­—)
   - `summary`: æ¸…æ™°ã€ç²¾ç‚¼çš„æ‘˜è¦ï¼Œ**åŠ¡å¿…å¿ å®äºåŸæ–‡** (3-5å¥è¯)
   - `context_ref`: æŒ‡å‘åŸæ–‡å…³é”®ä½ç½®çš„å¼•ç”¨
   - `key_formulas`: æ ¸å¿ƒå…¬å¼åˆ—è¡¨ (ç”¨LaTeXè¡¨ç¤º)
   - `key_terms`: æœ€æ ¸å¿ƒçš„1-3ä¸ªæœ¯è¯­
   - `difficulty_level`: éš¾åº¦ç­‰çº§ ("åŸºç¡€"/"è¿›é˜¶"/"é«˜çº§")
   - `knowledge_type`: çŸ¥è¯†ç‚¹ç±»å‹ (ä»ä»¥ä¸‹é€‰æ‹©: {knowledge_types})

**è¾“å‡ºè¦æ±‚ï¼š**
ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
```json
{{
  "knowledge_points": [
    {{
      "id": 1,
      "title": "çŸ¥è¯†ç‚¹æ ‡é¢˜",
      "summary": "çŸ¥è¯†ç‚¹çš„æ ¸å¿ƒæ‘˜è¦...",
      "context_ref": "åŸæ–‡ä½ç½®æè¿°",
      "key_formulas": ["$formula1$"],
      "key_terms": ["æœ¯è¯­A"],
      "difficulty_level": "åŸºç¡€",
      "knowledge_type": "æ¦‚å¿µå®šä¹‰"
    }}
  ]
}}
```"""

    def _get_specific_instructions(self, quality_level: str) -> str:
        """æ ¹æ®è´¨é‡æ¡£ä½ç”Ÿæˆå…·ä½“çš„è¯†åˆ«æŒ‡ä»¤"""
        instructions = {
            "ç®€çº¦": """
   - åªè¯†åˆ«æœ€æ ¸å¿ƒçš„æ¦‚å¿µå®šä¹‰å’ŒåŸºæœ¬åŸç†
   - æ¯ä¸ªæ–‡æœ¬å—æœ€å¤šæå–3ä¸ªæœ€é‡è¦çš„çŸ¥è¯†ç‚¹
   - å¿½ç•¥å…·ä½“ä¾‹å­å’Œç»†èŠ‚è¯´æ˜
   - ä¸“æ³¨äº"æ˜¯ä»€ä¹ˆ"è€Œé"æ€ä¹ˆåš"
            """,
            "ä¸­ç­‰": """
   - è¯†åˆ«é‡è¦çš„æ¦‚å¿µã€åŸç†ã€æ–¹æ³•å’Œå…³é”®ä¾‹å­
   - æ¯ä¸ªæ–‡æœ¬å—æå–é‡è¦çŸ¥è¯†ç‚¹ï¼Œå‚è€ƒ3-5ä¸ªï¼Œä»¥å®é™…ä¸ºä¸»
   - åŒ…å«å¿…è¦çš„å…¬å¼å’Œè®¡ç®—æ–¹æ³•
   - é€‚å½“åŒ…å«é‡è¦çš„åº”ç”¨åœºæ™¯
            """,
            "è¾ƒç»†è‡´": """
   - è¯¦ç»†è¯†åˆ«æ¦‚å¿µã€åŸç†ã€æ–¹æ³•ã€ä¾‹å­å’Œæ³¨æ„äº‹é¡¹
   - æ¯ä¸ªæ–‡æœ¬å—æå–çŸ¥è¯†ç‚¹ï¼Œå‚è€ƒ5-7ä¸ªï¼Œä»¥å®é™…æœ‰æ„ä¹‰çŸ¥è¯†ç‚¹æ•°é‡ä¸ºä¸»
   - æ·±å…¥æŒ–æ˜éšå«çš„é€»è¾‘å…³ç³»å’Œæ¡ä»¶
   - è¯†åˆ«é‡è¦çš„å¯¹æ¯”å’Œåˆ†ç±»ä¿¡æ¯
   - åŒ…å«å…·ä½“çš„æ“ä½œæ­¥éª¤å’Œåº”ç”¨åœºæ™¯
            """,
            "ç»†è‡´": """
   - å…¨é¢è¯†åˆ«æ‰€æœ‰é‡è¦çš„çŸ¥è¯†è¦ç´ 
   - æ¯ä¸ªæ–‡æœ¬å—æå–çŸ¥è¯†ç‚¹ï¼Œå‚è€ƒ7-10ä¸ªï¼Œä»¥å®é™…æœ‰æ„ä¹‰çŸ¥è¯†ç‚¹æ•°é‡ä¸ºä¸»
   - è¯†åˆ«å‰ææ¡ä»¶ã€é™åˆ¶æ¡ä»¶å’Œé€‚ç”¨èŒƒå›´
   - æå–é‡è¦çš„æ³¨æ„äº‹é¡¹å’Œå¸¸è§è¯¯åŒº
   - åŒ…å«è¯¦ç»†çš„ä¾‹å­å’Œå¯¹æ¯”åˆ†æ
   - è¯†åˆ«çŸ¥è¯†ç‚¹ä¹‹é—´çš„å…³ç³»å’Œä¾èµ–
            """,
            "ç²¾ç»†": """
   - æå…¶è¯¦ç»†åœ°è¯†åˆ«æ‰€æœ‰å¯èƒ½çš„çŸ¥è¯†ç‚¹
   - æ¯ä¸ªæ–‡æœ¬å—æå–çŸ¥è¯†ç‚¹ï¼Œå‚è€ƒ10-15ä¸ªï¼Œä»¥å®é™…æœ‰æ„ä¹‰çŸ¥è¯†ç‚¹æ•°é‡ä¸ºä¸»
   - æå–æ¯ä¸ªé‡è¦çš„ç»†èŠ‚å’Œè¡¥å……è¯´æ˜
   - è¯†åˆ«éšå«çš„å‡è®¾å’Œè¾¹ç•Œæ¡ä»¶
   - åŒ…å«èƒŒæ™¯ä¿¡æ¯å’Œæ‰©å±•çŸ¥è¯†
   - è¯¦ç»†çš„æ­¥éª¤åˆ†è§£å’Œå¤šè§’åº¦åˆ†æ
   - è¯†åˆ«æ½œåœ¨çš„åº”ç”¨åœºæ™¯å’Œé™åˆ¶
            """
        }
        return instructions.get(quality_level, instructions["ä¸­ç­‰"])

    async def extract_from_chunk(self, chunk_text: str, chunk_metadata: Dict) -> List[KnowledgePoint]:
        """ä»å•ä¸ªæ–‡æœ¬å—ä¸­æå–çŸ¥è¯†ç‚¹"""
        specific_instructions = self._get_specific_instructions(self.quality_level)
        knowledge_types_str = '", "'.join(self.strategy["knowledge_types"])

        prompt = self.extraction_prompt_template.format(
            quality_level=self.quality_level,
            extraction_focus=self.strategy["focus"],
            detail_level=self.strategy["detail_level"],
            knowledge_types=knowledge_types_str,
            chunk_index=chunk_metadata['chunk_index'] + 1,
            chunk_text=chunk_text,
            specific_instructions=specific_instructions
        )

        response = await self.llm_client.call_api(prompt, max_tokens=4000)  # å¢åŠ tokenæ•°

        try:
            json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = response

            data = json.loads(json_str)
            knowledge_points = []

            for kp_data in data.get('knowledge_points', []):
                kp = KnowledgePoint(
                    id=kp_data['id'],
                    title=kp_data['title'],
                    summary=kp_data['summary'],
                    context_ref=kp_data.get('context_ref', ''),
                    key_formulas=kp_data.get('key_formulas', []),
                    key_terms=kp_data.get('key_terms', []),
                    difficulty_level=kp_data.get('difficulty_level', 'åŸºç¡€'),
                    knowledge_type=kp_data.get('knowledge_type', 'æ¦‚å¿µå®šä¹‰')
                )
                knowledge_points.append(kp)

            return knowledge_points
        except Exception as e:
            print(f"è§£æçŸ¥è¯†ç‚¹æ—¶å‡ºé”™: {e}")
            return []

    async def extract_all(self, chunks: List[Tuple[str, Dict]]) -> List[KnowledgePoint]:
        """ä»æ‰€æœ‰æ–‡æœ¬å—ä¸­æå–çŸ¥è¯†ç‚¹"""
        all_knowledge_points = []

        # å¹¶å‘å¤„ç†å¤šä¸ªå—
        tasks = []
        for chunk_text, chunk_metadata in chunks:
            task = self.extract_from_chunk(chunk_text, chunk_metadata)
            tasks.append(task)

        # ä½¿ç”¨è¿›åº¦æ¡
        results = []
        for f in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="æå–çŸ¥è¯†ç‚¹"):
            result = await f
            results.extend(result)

        return results

class KnowledgePointMerger:
    """çŸ¥è¯†ç‚¹åˆå¹¶å™¨"""

    def __init__(self, quality_level: str = "ä¸­ç­‰"):
        # æ ¹æ®è´¨é‡æ¡£ä½è®¾ç½®ä¸åŒçš„ç›¸ä¼¼åº¦é˜ˆå€¼å’Œå»é‡ç­–ç•¥
        configs = {
            "ç®€çº¦": {"similarity_threshold": 0.60, "title_threshold": 0.70, "aggressive_merge": True},
            "ä¸­ç­‰": {"similarity_threshold": 0.72, "title_threshold": 0.80, "aggressive_merge": False},
            "è¾ƒç»†è‡´": {"similarity_threshold": 0.80, "title_threshold": 0.85, "aggressive_merge": False},
            "ç»†è‡´": {"similarity_threshold": 0.85, "title_threshold": 0.90, "aggressive_merge": False},
            "ç²¾ç»†": {"similarity_threshold": 0.90, "title_threshold": 0.95, "aggressive_merge": False},
        }

        config = configs.get(quality_level, configs["ä¸­ç­‰"])
        self.similarity_threshold = config["similarity_threshold"]
        self.title_threshold = config["title_threshold"]
        self.aggressive_merge = config["aggressive_merge"]
        self.quality_level = quality_level

    def merge_knowledge_points(self, knowledge_points: List[KnowledgePoint]) -> List[KnowledgePoint]:
        """åˆå¹¶ç›¸ä¼¼çš„çŸ¥è¯†ç‚¹ï¼Œä½¿ç”¨å¤šå±‚å»é‡ç­–ç•¥"""
        if not knowledge_points:
            return []

        print(f"ğŸ”„ å¼€å§‹çŸ¥è¯†ç‚¹å»é‡åˆå¹¶ (è´¨é‡æ¡£ä½: {self.quality_level})")
        print(f"   åŸå§‹çŸ¥è¯†ç‚¹æ•°: {len(knowledge_points)}")

        # ç¬¬ä¸€æ­¥ï¼šåŸºäºæ ‡é¢˜çš„ç²¾ç¡®å»é‡
        deduped_by_title = self._deduplicate_by_title(knowledge_points)
        print(f"   æ ‡é¢˜å»é‡å: {len(deduped_by_title)}")

        # ç¬¬äºŒæ­¥ï¼šåŸºäºå†…å®¹ç›¸ä¼¼åº¦çš„åˆå¹¶
        final_kps = self._merge_by_similarity(deduped_by_title)
        print(f"   ç›¸ä¼¼åº¦åˆå¹¶å: {len(final_kps)}")

        # ç¬¬ä¸‰æ­¥ï¼šè´¨é‡è¿‡æ»¤å’Œä¼˜åŒ–
        optimized_kps = self._optimize_knowledge_points(final_kps)
        print(f"   è´¨é‡ä¼˜åŒ–å: {len(optimized_kps)}")

        # é‡æ–°ç¼–å·
        for i, kp in enumerate(optimized_kps):
            kp.id = i + 1

        return optimized_kps

    def _deduplicate_by_title(self, kps: List[KnowledgePoint]) -> List[KnowledgePoint]:
        """åŸºäºæ ‡é¢˜çš„ç²¾ç¡®å»é‡"""
        from difflib import SequenceMatcher

        unique_kps = []
        for kp in kps:
            is_duplicate = False
            for existing_kp in unique_kps:
                # è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦
                title_sim = SequenceMatcher(None, kp.title, existing_kp.title).ratio()
                if title_sim > self.title_threshold:
                    # åˆå¹¶åˆ°ç°æœ‰çŸ¥è¯†ç‚¹
                    existing_kp.summary = self._merge_summaries(existing_kp.summary, kp.summary)
                    existing_kp.key_formulas = list(set(existing_kp.key_formulas + kp.key_formulas))
                    existing_kp.key_terms = list(set(existing_kp.key_terms + kp.key_terms))
                    is_duplicate = True
                    break

            if not is_duplicate:
                unique_kps.append(kp)

        return unique_kps

    def _merge_by_similarity(self, kps: List[KnowledgePoint]) -> List[KnowledgePoint]:
        """åŸºäºå†…å®¹ç›¸ä¼¼åº¦çš„åˆå¹¶"""
        if len(kps) <= 1:
            return kps

        # æ„å»ºæ–‡æœ¬å‘é‡
        texts = [f"{kp.title} {kp.summary} {' '.join(kp.key_terms)}" for kp in kps]

        try:
            vectorizer = TfidfVectorizer(max_features=200, stop_words=None)
            tfidf_matrix = vectorizer.fit_transform(texts)
            similarity_matrix = cosine_similarity(tfidf_matrix)
        except:
            # å¦‚æœå‘é‡åŒ–å¤±è´¥ï¼Œè¿”å›åŸå§‹åˆ—è¡¨
            return kps

        # æ ‡è®°å·²åˆå¹¶çš„çŸ¥è¯†ç‚¹
        merged = [False] * len(kps)
        merged_kps = []

        for i in range(len(kps)):
            if merged[i]:
                continue

            # æ‰¾åˆ°ç›¸ä¼¼çš„çŸ¥è¯†ç‚¹
            similar_indices = []
            for j in range(i + 1, len(kps)):
                if not merged[j] and similarity_matrix[i][j] > self.similarity_threshold:
                    # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿çŸ¥è¯†ç‚¹ç±»å‹ç›¸åŒæˆ–ç›¸å…³
                    if self._are_types_compatible(kps[i].knowledge_type, kps[j].knowledge_type):
                        similar_indices.append(j)

            # åˆå¹¶çŸ¥è¯†ç‚¹
            if similar_indices and self.aggressive_merge:
                merged_kp = self._merge_multiple([kps[i]] + [kps[j] for j in similar_indices])
                merged_kps.append(merged_kp)
                merged[i] = True
                for j in similar_indices:
                    merged[j] = True
            else:
                merged_kps.append(kps[i])
                merged[i] = True

        return merged_kps

    def _are_types_compatible(self, type1: str, type2: str) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªçŸ¥è¯†ç‚¹ç±»å‹æ˜¯å¦å¯ä»¥åˆå¹¶"""
        # å®šä¹‰å¯ä»¥åˆå¹¶çš„çŸ¥è¯†ç‚¹ç±»å‹ç»„åˆ
        compatible_groups = [
            ["æ¦‚å¿µå®šä¹‰", "èƒŒæ™¯ä¿¡æ¯"],
            ["åŸç†æ–¹æ³•", "å®ä¾‹åº”ç”¨"],
            ["å…¬å¼è®¡ç®—", "åŸç†æ–¹æ³•"],
            ["æ³¨æ„äº‹é¡¹", "æ¡ä»¶é™åˆ¶"],
            ["å¯¹æ¯”åˆ†æ", "æ¦‚å¿µå®šä¹‰"]
        ]

        if type1 == type2:
            return True

        for group in compatible_groups:
            if type1 in group and type2 in group:
                return True

        return False

    def _merge_summaries(self, summary1: str, summary2: str) -> str:
        """æ™ºèƒ½åˆå¹¶ä¸¤ä¸ªæ‘˜è¦"""
        sentences1 = [s.strip() for s in summary1.split('ã€‚') if s.strip()]
        sentences2 = [s.strip() for s in summary2.split('ã€‚') if s.strip()]

        # å»é™¤é‡å¤å¥å­
        unique_sentences = []
        all_sentences = sentences1 + sentences2

        for sentence in all_sentences:
            if sentence and not any(self._sentence_similarity(sentence, existing) > 0.8
                                  for existing in unique_sentences):
                unique_sentences.append(sentence)

        # é™åˆ¶é•¿åº¦ï¼Œä¼˜å…ˆä¿ç•™æ›´æœ‰ä¿¡æ¯é‡çš„å¥å­
        if len(unique_sentences) > 5:
            unique_sentences = sorted(unique_sentences, key=len, reverse=True)[:5]

        return 'ã€‚'.join(unique_sentences) + 'ã€‚'

    def _sentence_similarity(self, sent1: str, sent2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼åº¦"""
        from difflib import SequenceMatcher
        return SequenceMatcher(None, sent1, sent2).ratio()

    def _optimize_knowledge_points(self, kps: List[KnowledgePoint]) -> List[KnowledgePoint]:
        """ä¼˜åŒ–çŸ¥è¯†ç‚¹è´¨é‡"""
        optimized = []

        for kp in kps:
            # è¿‡æ»¤æ‰è´¨é‡è¿‡ä½çš„çŸ¥è¯†ç‚¹
            if len(kp.title) < 3 or len(kp.summary) < 10:
                continue

            # æ¸…ç†å’Œä¼˜åŒ–å†…å®¹
            kp.title = kp.title.strip()
            kp.summary = kp.summary.strip()
            kp.key_formulas = [f.strip() for f in kp.key_formulas if f.strip()]
            kp.key_terms = [t.strip() for t in kp.key_terms if t.strip()]

            optimized.append(kp)

        return optimized

    def _merge_multiple(self, kps: List[KnowledgePoint]) -> KnowledgePoint:
        """åˆå¹¶å¤šä¸ªçŸ¥è¯†ç‚¹"""
        # é€‰æ‹©æœ€é•¿çš„æ ‡é¢˜
        title = max([kp.title for kp in kps], key=len)

        # æ™ºèƒ½åˆå¹¶æ‘˜è¦
        summaries = [kp.summary for kp in kps]
        merged_summary = self._merge_summaries(summaries[0],
                                             'ã€‚'.join(summaries[1:]) if len(summaries) > 1 else "")

        # åˆå¹¶å…¶ä»–å­—æ®µ
        context_refs = [kp.context_ref for kp in kps if kp.context_ref]
        key_formulas = list(set(sum([kp.key_formulas for kp in kps], [])))
        key_terms = list(set(sum([kp.key_terms for kp in kps], [])))

        # é€‰æ‹©æœ€é«˜çš„éš¾åº¦çº§åˆ«
        difficulty_levels = [kp.difficulty_level for kp in kps]
        difficulty_order = {"åŸºç¡€": 1, "è¿›é˜¶": 2, "é«˜çº§": 3}
        final_difficulty = max(difficulty_levels, key=lambda x: difficulty_order.get(x, 1))

        return KnowledgePoint(
            id=kps[0].id,
            title=title,
            summary=merged_summary,
            context_ref='; '.join(context_refs),
            key_formulas=key_formulas,
            key_terms=key_terms,
            difficulty_level=final_difficulty,
            knowledge_type=kps[0].knowledge_type  # ä½¿ç”¨ç¬¬ä¸€ä¸ªçš„ç±»å‹
        )

class QuestionGenerator:
    """å¢å¼ºç‰ˆé¢˜ç›®ç”Ÿæˆå™¨"""

    def __init__(self, llm_client: LLMClient, quality_level: str = "ä¸­ç­‰"):
        self.llm_client = llm_client
        self.quality_level = quality_level

        # æ ¹æ®è´¨é‡æ¡£ä½é…ç½®é¢˜ç›®ç”Ÿæˆç­–ç•¥
        self.question_configs = {
            "ç®€çº¦": {"basic_per_kp": 1, "fusion_ratio": 0.1, "advanced_ratio": 0.1},
            "ä¸­ç­‰": {"basic_per_kp": 1, "fusion_ratio": 0.2, "advanced_ratio": 0.2},
            "è¾ƒç»†è‡´": {"basic_per_kp": 1, "fusion_ratio": 0.3, "advanced_ratio": 0.3},
            "ç»†è‡´": {"basic_per_kp": 1, "fusion_ratio": 0.4, "advanced_ratio": 0.4},
            "ç²¾ç»†": {"basic_per_kp": 1, "fusion_ratio": 0.5, "advanced_ratio": 0.5}
        }

        self.basic_question_template = """è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹æä¾›çš„å•ä¸€çŸ¥è¯†ç‚¹æ‘˜è¦ï¼Œç”Ÿæˆä¸€é“é«˜è´¨é‡å››é€‰é¡¹å•é¡¹é€‰æ‹©é¢˜ã€‚

çŸ¥è¯†ç‚¹ä¿¡æ¯:
- æ ‡é¢˜: {title}
- éš¾åº¦: {difficulty_level}
- ç±»å‹: {knowledge_type}
- æ ¸å¿ƒæ‘˜è¦: {summary}
{formulas_text}

é¢˜ç›®è¦æ±‚:
1. é¢˜ç›®è¦å‡†ç¡®æµ‹è¯•å¯¹è¯¥çŸ¥è¯†ç‚¹çš„{test_focus}
2. æ­£ç¡®ç­”æ¡ˆå¿…é¡»å®Œå…¨åŸºäºæ‘˜è¦å†…å®¹
3. ä¸‰ä¸ªé”™è¯¯é€‰é¡¹è¦æœ‰è¿·æƒ‘æ€§ä½†æ˜æ˜¾é”™è¯¯
4. é¢˜ç›®éš¾åº¦ä¸º: {target_difficulty}
5. å¦‚æœæ¶‰åŠå…¬å¼ï¼Œä½¿ç”¨LaTeXæ ¼å¼ $formula$
6. **é‡è¦ï¼šæ­£ç¡®ç­”æ¡ˆéšæœºåˆ†å¸ƒåœ¨Aã€Bã€Cã€Dä¸­**

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰:
```json
{{
  "question": "é¢˜ç›®å†…å®¹",
  "options": {{
    "A": "é€‰é¡¹Aå†…å®¹",
    "B": "é€‰é¡¹Bå†…å®¹",
    "C": "é€‰é¡¹Cå†…å®¹",
    "D": "é€‰é¡¹Då†…å®¹"
  }},
  "correct_answer": "A/B/C/D",
  "explanation": "ç­”æ¡ˆè§£é‡Š",
  "difficulty": "{target_difficulty}",
  "question_type": "åŸºç¡€ç†è§£"
}}
```"""

        self.fusion_question_template = """è¯·æ ¹æ®ä»¥ä¸‹å¤šä¸ªç›¸å…³çŸ¥è¯†ç‚¹ï¼Œç”Ÿæˆä¸€é“èåˆæ€§å››é€‰é¡¹å•é¡¹é€‰æ‹©é¢˜ï¼Œè¦æ±‚ç»¼åˆè¿ç”¨å¤šä¸ªçŸ¥è¯†ç‚¹ã€‚

ç›¸å…³çŸ¥è¯†ç‚¹:
{knowledge_points_info}

é¢˜ç›®è¦æ±‚:
1. é¢˜ç›®éœ€è¦ç»¼åˆè¿ç”¨ä¸Šè¿°å¤šä¸ªçŸ¥è¯†ç‚¹æ‰èƒ½æ­£ç¡®å›ç­”
2. é¢˜å‹ä¸º: {question_type}
3. éš¾åº¦ç­‰çº§: {target_difficulty}
4. ä¸‰ä¸ªé”™è¯¯é€‰é¡¹è¦åŸºäºéƒ¨åˆ†çŸ¥è¯†ç‚¹ä½†ç»“è®ºé”™è¯¯
5. æ­£ç¡®ç­”æ¡ˆéšæœºåˆ†å¸ƒåœ¨Aã€Bã€Cã€Dä¸­

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰:
```json
{{
  "question": "é¢˜ç›®å†…å®¹",
  "options": {{
    "A": "é€‰é¡¹Aå†…å®¹",
    "B": "é€‰é¡¹Bå†…å®¹",
    "C": "é€‰é¡¹Cå†…å®¹",
    "D": "é€‰é¡¹Då†…å®¹"
  }},
  "correct_answer": "A/B/C/D",
  "explanation": "ç»¼åˆè§£é‡Šï¼Œè¯´æ˜æ¶‰åŠçš„å¤šä¸ªçŸ¥è¯†ç‚¹",
  "difficulty": "{target_difficulty}",
  "question_type": "{question_type}",
  "related_knowledge_points": {related_kp_ids}
}}
```"""

    async def generate_basic_question(self, kp: KnowledgePoint, question_id: int, target_difficulty: str = "medium") -> Optional[Question]:
        """ä¸ºå•ä¸ªçŸ¥è¯†ç‚¹ç”ŸæˆåŸºç¡€é¢˜ç›®"""
        import random

        # æ ¹æ®çŸ¥è¯†ç‚¹ç±»å‹ç¡®å®šæµ‹è¯•é‡ç‚¹
        test_focus_map = {
            "æ¦‚å¿µå®šä¹‰": "æ¦‚å¿µç†è§£å’Œå®šä¹‰è®°å¿†",
            "åŸç†æ–¹æ³•": "åŸç†ç†è§£å’Œæ–¹æ³•åº”ç”¨",
            "å…¬å¼è®¡ç®—": "å…¬å¼ç†è§£å’Œè®¡ç®—èƒ½åŠ›",
            "å®ä¾‹åº”ç”¨": "å®é™…åº”ç”¨å’Œæ¡ˆä¾‹åˆ†æ",
            "æ³¨æ„äº‹é¡¹": "æ³¨æ„äº‹é¡¹å’Œé™åˆ¶æ¡ä»¶çš„ç†è§£"
        }

        test_focus = test_focus_map.get(kp.knowledge_type, "æ ¸å¿ƒç†è§£")

        formulas_text = ""
        if kp.key_formulas:
            formulas_text = f"ç›¸å…³å…¬å¼: {', '.join(kp.key_formulas)}"

        prompt = self.basic_question_template.format(
            title=kp.title,
            difficulty_level=kp.difficulty_level,
            knowledge_type=kp.knowledge_type,
            summary=kp.summary,
            formulas_text=formulas_text,
            test_focus=test_focus,
            target_difficulty=target_difficulty
        )

        response = await self.llm_client.call_api(prompt)
        return self._parse_question_response(response, question_id, kp.id)

    async def generate_fusion_question(self, kps: List[KnowledgePoint], question_id: int, question_type: str) -> Optional[Question]:
        """ç”Ÿæˆèåˆå¤šä¸ªçŸ¥è¯†ç‚¹çš„é¢˜ç›®"""

        # æ„å»ºçŸ¥è¯†ç‚¹ä¿¡æ¯
        kp_info_list = []
        for i, kp in enumerate(kps, 1):
            kp_info = f"{i}. {kp.title}: {kp.summary}"
            if kp.key_formulas:
                kp_info += f" (å…¬å¼: {', '.join(kp.key_formulas)})"
            kp_info_list.append(kp_info)

        knowledge_points_info = "\n".join(kp_info_list)
        related_kp_ids = [kp.id for kp in kps]

        # æ ¹æ®æ¶‰åŠçŸ¥è¯†ç‚¹æ•°é‡å’Œç±»å‹ç¡®å®šéš¾åº¦
        target_difficulty = "hard" if len(kps) >= 3 else "medium"

        prompt = self.fusion_question_template.format(
            knowledge_points_info=knowledge_points_info,
            question_type=question_type,
            target_difficulty=target_difficulty,
            related_kp_ids=related_kp_ids
        )

        response = await self.llm_client.call_api(prompt)
        question = self._parse_question_response(response, question_id, kps[0].id)

        if question:
            question.related_knowledge_points = related_kp_ids

        return question

    def _parse_question_response(self, response: str, question_id: int, kp_id: int) -> Optional[Question]:
        """è§£æé¢˜ç›®ç”Ÿæˆå“åº”"""
        import random
        try:
            # æå–JSONå†…å®¹
            json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = response

            json_str = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', json_str)

            json_str = json_str.replace('\b', '').replace('\f', '').replace('\v', '')

            json_str = json_str.replace('\\', '\\\\').replace('\\"', '"')

            data = json.loads(json_str)

            # éšæœºåŒ–ç­”æ¡ˆä½ç½®
            options = data['options']
            correct_answer = data['correct_answer']

            correct_content = options[correct_answer]

            option_contents = list(options.values())
            random.shuffle(option_contents)
            new_options = {}
            new_correct_answer = None
            option_keys = ['A', 'B', 'C', 'D']

            for i, content in enumerate(option_contents):
                new_options[option_keys[i]] = content
                if content == correct_content:
                    new_correct_answer = option_keys[i]

            question = Question(
                id=question_id,
                knowledge_point_id=kp_id,
                question=data['question'],
                options=new_options,
                correct_answer=new_correct_answer,
                explanation=data['explanation'],
                difficulty=data.get('difficulty', 'medium')
            )

            question.question_type = data.get('question_type', 'åŸºç¡€ç†è§£')
            question.related_knowledge_points = data.get('related_knowledge_points', [kp_id])

            return question

        except Exception as e:
            print(f"ç”Ÿæˆé¢˜ç›®æ—¶å‡ºé”™: {e}")
            return None

    async def generate_all(self, knowledge_points: List[KnowledgePoint]) -> List[Question]:
        """ä¸ºæ‰€æœ‰çŸ¥è¯†ç‚¹ç”Ÿæˆå®Œæ•´çš„é¢˜ç›®é›†"""
        config = self.question_configs.get(self.quality_level, self.question_configs["ä¸­ç­‰"])
        questions = []
        question_id = 1

        print(f"ğŸ“Š é¢˜ç›®ç”Ÿæˆç­–ç•¥ - åŸºç¡€é¢˜:{config['basic_per_kp']}é¢˜/çŸ¥è¯†ç‚¹, èåˆé¢˜æ¯”ä¾‹:{config['fusion_ratio']:.0%}, é«˜éš¾åº¦æ¯”ä¾‹:{config['advanced_ratio']:.0%}")

        # 1. ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹ç”ŸæˆåŸºç¡€é¢˜ç›®
        basic_tasks = []
        for kp in knowledge_points:
            for _ in range(config['basic_per_kp']):
                difficulty = "hard" if kp.difficulty_level == "é«˜çº§" else "medium" if kp.difficulty_level == "è¿›é˜¶" else "easy"
                task = self.generate_basic_question(kp, question_id, difficulty)
                basic_tasks.append(task)
                question_id += 1

        # ç”ŸæˆåŸºç¡€é¢˜ç›®
        for f in tqdm(asyncio.as_completed(basic_tasks), total=len(basic_tasks), desc="ç”ŸæˆåŸºç¡€é¢˜ç›®"):
            question = await f
            if question:
                questions.append(question)

        # 2. ç”Ÿæˆèåˆé¢˜ç›®
        fusion_count = int(len(knowledge_points) * config['fusion_ratio'])
        fusion_types = [
            "å› æœæ¨ç†å‹", "ç»¼åˆå…³è”å‹", "æœ€ä¼˜æ–¹æ¡ˆå‹", "æƒ…æ™¯åº”ç”¨é¢˜",
            "å¯¹æ¯”åˆ†æå‹", "ç»¼åˆåˆ¤æ–­å‹", "ç³»ç»Ÿåˆ†æå‹"
        ]

        fusion_tasks = []
        for _ in range(fusion_count):
            # éšæœºé€‰æ‹©2-4ä¸ªç›¸å…³çŸ¥è¯†ç‚¹
            import random
            selected_kps = random.sample(knowledge_points, min(random.randint(2, 4), len(knowledge_points)))
            question_type = random.choice(fusion_types)

            task = self.generate_fusion_question(selected_kps, question_id, question_type)
            fusion_tasks.append(task)
            question_id += 1

        # ç”Ÿæˆèåˆé¢˜ç›®
        for f in tqdm(asyncio.as_completed(fusion_tasks), total=len(fusion_tasks), desc="ç”Ÿæˆèåˆé¢˜ç›®"):
            question = await f
            if question:
                questions.append(question)

        # 3. ç”Ÿæˆé¢å¤–çš„é«˜éš¾åº¦é¢˜ç›®
        advanced_count = int(len(knowledge_points) * config['advanced_ratio'])
        advanced_tasks = []

        # é€‰æ‹©éš¾åº¦è¾ƒé«˜çš„çŸ¥è¯†ç‚¹
        advanced_kps = [kp for kp in knowledge_points if kp.difficulty_level in ["è¿›é˜¶", "é«˜çº§"]]
        if not advanced_kps:
            advanced_kps = knowledge_points  # å¦‚æœæ²¡æœ‰é«˜éš¾åº¦çŸ¥è¯†ç‚¹ï¼Œä½¿ç”¨æ‰€æœ‰çŸ¥è¯†ç‚¹

        for _ in range(advanced_count):
            import random
            kp = random.choice(advanced_kps)
            task = self.generate_basic_question(kp, question_id, "hard")
            advanced_tasks.append(task)
            question_id += 1

        # ç”Ÿæˆé«˜éš¾åº¦é¢˜ç›®
        for f in tqdm(asyncio.as_completed(advanced_tasks), total=len(advanced_tasks), desc="ç”Ÿæˆé«˜éš¾åº¦é¢˜ç›®"):
            question = await f
            if question:
                questions.append(question)

        # æŒ‰IDæ’åº
        questions.sort(key=lambda q: q.id)
        return questions

class NoteToQuizGenerator:
    """ç¬”è®°ç”Ÿé¢˜å™¨ä¸»ç±»"""

    def __init__(self, api_key: str, quality_level: str = "ä¸­ç­‰"):
        self.llm_client = LLMClient(api_key)
        self.chunker = TextChunker(quality_level)
        self.extractor = KnowledgeExtractor(self.llm_client, quality_level)
        self.merger = KnowledgePointMerger(quality_level)
        self.generator = QuestionGenerator(self.llm_client, quality_level)

    async def process_document(self, file_path: str) -> Tuple[List[KnowledgePoint], List[Question]]:
        """å¤„ç†æ–‡æ¡£å¹¶ç”Ÿæˆé¢˜ç›®"""
        print("ğŸ“„ æ­£åœ¨è§£ææ–‡æ¡£...")
        text = DocumentParser.parse_document(file_path)

        print("âœ‚ï¸ æ­£åœ¨åˆ†å—å¤„ç†...")
        chunks = self.chunker.chunk_text(text)
        print(f"æ–‡æ¡£è¢«åˆ†æˆ {len(chunks)} ä¸ªå—")

        print("ğŸ” æ­£åœ¨æå–çŸ¥è¯†ç‚¹...")
        raw_knowledge_points = await self.extractor.extract_all(chunks)
        print(f"æå–åˆ° {len(raw_knowledge_points)} ä¸ªåŸå§‹çŸ¥è¯†ç‚¹")

        print("ğŸ”— æ­£åœ¨åˆå¹¶ç›¸ä¼¼çŸ¥è¯†ç‚¹...")
        merged_knowledge_points = self.merger.merge_knowledge_points(raw_knowledge_points)
        print(f"åˆå¹¶åå‰©ä½™ {len(merged_knowledge_points)} ä¸ªçŸ¥è¯†ç‚¹")

        print("ğŸ“ æ­£åœ¨ç”Ÿæˆé¢˜ç›®...")
        questions = await self.generator.generate_all(merged_knowledge_points)
        print(f"æˆåŠŸç”Ÿæˆ {len(questions)} é“é¢˜ç›®")

        return merged_knowledge_points, questions

    def save_results(self, knowledge_points: List[KnowledgePoint],
                    questions: List[Question],
                    output_dir: str = "output"):
        """ä¿å­˜ç»“æœ"""
        os.makedirs(output_dir, exist_ok=True)

        # ä¿å­˜çŸ¥è¯†ç‚¹
        kp_data = [asdict(kp) for kp in knowledge_points]
        with open(f"{output_dir}/knowledge_points.json", 'w', encoding='utf-8') as f:
            json.dump(kp_data, f, ensure_ascii=False, indent=2)

        # ä¿å­˜é¢˜ç›®
        q_data = [asdict(q) for q in questions]
        with open(f"{output_dir}/questions.json", 'w', encoding='utf-8') as f:
            json.dump(q_data, f, ensure_ascii=False, indent=2)

        # ç”Ÿæˆå¯æ‰“å°çš„é¢˜ç›®æ–‡æ¡£
        with open(f"{output_dir}/quiz.txt", 'w', encoding='utf-8') as f:
            f.write("=== ç”Ÿæˆçš„æµ‹éªŒé¢˜ç›® ===\n\n")
            for q in questions:
                f.write(f"é¢˜ç›® {q.id}. {q.question}\n")
                for opt, content in q.options.items():
                    f.write(f"  {opt}. {content}\n")
                f.write(f"\n")

            f.write("\n\n=== ç­”æ¡ˆå’Œè§£é‡Š ===\n\n")
            for q in questions:
                f.write(f"é¢˜ç›® {q.id}: {q.correct_answer}\n")
                f.write(f"è§£é‡Š: {q.explanation}\n\n")

        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ° {output_dir} ç›®å½•")


class InteractiveReviewer:
    """äº¤äº’å¼çŸ¥è¯†ç‚¹å®¡æ ¸å™¨"""

    def review_knowledge_points(self, knowledge_points: List[KnowledgePoint]) -> List[KnowledgePoint]:
        """è®©ç”¨æˆ·å®¡æ ¸å’Œç¼–è¾‘çŸ¥è¯†ç‚¹"""
        print("\nğŸ“‹ çŸ¥è¯†ç‚¹å®¡æ ¸")
        print("=" * 50)

        reviewed_kps = []

        for i, kp in enumerate(knowledge_points):
            print(f"\nçŸ¥è¯†ç‚¹ {i+1}/{len(knowledge_points)}")
            print(f"æ ‡é¢˜: {kp.title}")
            print(f"æ‘˜è¦: {kp.summary}")
            if kp.key_formulas:
                print(f"å…¬å¼: {', '.join(kp.key_formulas)}")

            action = input("\næ“ä½œ: [K]ä¿ç•™ [E]ç¼–è¾‘ [D]åˆ é™¤ ï¼š").upper()

            if action == 'K':
                reviewed_kps.append(kp)
            elif action == 'E':
                new_title = input(f"æ–°æ ‡é¢˜ (å›è½¦ä¿æŒåŸæ ‡é¢˜): ").strip()
                new_summary = input(f"æ–°æ‘˜è¦ (å›è½¦ä¿æŒåŸæ‘˜è¦): ").strip()

                if new_title:
                    kp.title = new_title
                if new_summary:
                    kp.summary = new_summary

                reviewed_kps.append(kp)
            # D - åˆ é™¤ï¼Œä¸æ·»åŠ åˆ°reviewed_kps

        for i, kp in enumerate(reviewed_kps):
            kp.id = i + 1

        print(f"\nâœ… å®¡æ ¸å®Œæˆï¼ä¿ç•™äº† {len(reviewed_kps)} ä¸ªçŸ¥è¯†ç‚¹")
        return reviewed_kps

# ========== æ‰¹é‡å¤„ç†åŠŸèƒ½ ==========

class BatchProcessor:
    """æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æ¡£"""

    def __init__(self, generator: NoteToQuizGenerator):
        self.generator = generator

    async def process_multiple_documents(self, file_paths: List[str]) -> Dict[str, Tuple[List[KnowledgePoint], List[Question]]]:
        """æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æ¡£"""
        results = {}

        for file_path in file_paths:
            print(f"\nå¤„ç†æ–‡æ¡£: {file_path}")
            try:
                kps, questions = await self.generator.process_document(file_path)
                results[file_path] = (kps, questions)
            except Exception as e:
                print(f"å¤„ç† {file_path} æ—¶å‡ºé”™: {e}")
                results[file_path] = ([], [])

        return results

# ========== é¢˜ç›®æ ¼å¼åŒ–è¾“å‡º ==========

class QuizFormatter:
    """é¢˜ç›®æ ¼å¼åŒ–å™¨"""

    @staticmethod
    def to_html(questions: List[Question], knowledge_points: List[KnowledgePoint]) -> str:
        """ç”ŸæˆHTMLæ ¼å¼çš„æµ‹éªŒ"""
        html = """
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>çŸ¥è¯†ç‚¹æµ‹éªŒ</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        .question {
            margin-bottom: 30px;
            padding: 20px;
            background-color: #f5f5f5;
            border-radius: 8px;
        }
        .question-number {
            font-weight: bold;
            color: #333;
        }
        .options {
            margin-top: 10px;
        }
        .option {
            margin: 5px 0;
            padding: 5px 10px;
        }
        .answer-section {
            margin-top: 50px;
            border-top: 2px solid #ccc;
            padding-top: 20px;
        }
        .answer {
            margin: 10px 0;
            padding: 10px;
            background-color: #e8f5e9;
            border-radius: 5px;
        }
        .formula {
            font-style: italic;
            color: #1976d2;
        }
        .knowledge-point {
            margin: 20px 0;
            padding: 15px;
            background-color: #f0f7ff;
            border-left: 4px solid #2196f3;
        }
    </style>
</head>
<body>
    <h1>çŸ¥è¯†ç‚¹æµ‹éªŒ</h1>

    <h2>çŸ¥è¯†ç‚¹æ¦‚è§ˆ</h2>
    <div class="knowledge-points">
"""

        # æ·»åŠ çŸ¥è¯†ç‚¹æ¦‚è§ˆ
        for kp in knowledge_points:
            html += f"""
        <div class="knowledge-point">
            <h3>{kp.id}. {kp.title}</h3>
            <p>{kp.summary}</p>
        </div>
"""

        html += """
    </div>

    <h2>æµ‹éªŒé¢˜ç›®</h2>
    <div class="questions">
"""

        
        for q in questions:
            html += f"""
        <div class="question">
            <div class="question-number">é¢˜ç›® {q.id}</div>
            <p>{q.question}</p>
            <div class="options">
"""
            for opt, content in q.options.items():
                html += f'                <div class="option">{opt}. {content}</div>\n'

            html += """            </div>
        </div>
"""

        
        html += """
    </div>

    <div class="answer-section">
        <h2>ç­”æ¡ˆä¸è§£æ</h2>
"""

        for q in questions:
            html += f"""
        <div class="answer">
            <strong>é¢˜ç›® {q.id}:</strong> {q.correct_answer}<br>
            <strong>è§£æ:</strong> {q.explanation}
        </div>
"""

        html += """
    </div>
</body>
</html>
"""
        return html

    @staticmethod
    def to_markdown(questions: List[Question], knowledge_points: List[KnowledgePoint]) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„æµ‹éªŒ"""
        md = "# çŸ¥è¯†ç‚¹æµ‹éªŒ\n\n"

        md += "## çŸ¥è¯†ç‚¹æ¦‚è§ˆ\n\n"
        for kp in knowledge_points:
            md += f"### {kp.id}. {kp.title}\n\n"
            md += f"{kp.summary}\n\n"
            if kp.key_formulas:
                md += f"**å…³é”®å…¬å¼ï¼š** {', '.join(kp.key_formulas)}\n\n"

        md += "## æµ‹éªŒé¢˜ç›®\n\n"
        for q in questions:
            md += f"### é¢˜ç›® {q.id}\n\n"
            md += f"{q.question}\n\n"
            for opt, content in q.options.items():
                md += f"- {opt}. {content}\n"
            md += "\n"

        md += "## ç­”æ¡ˆä¸è§£æ\n\n"
        for q in questions:
            md += f"**é¢˜ç›® {q.id}:** {q.correct_answer}\n\n"
            md += f"**è§£æï¼š** {q.explanation}\n\n"

        return md

class Config:
    """é…ç½®ç®¡ç†ç±»"""

    API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
    API_BASE_URL = "https://api.deepseek.com"

    QUALITY_LEVEL = "ä¸­ç­‰"

    MAX_CONCURRENT_REQUESTS = 5

    OUTPUT_DIR = "output"

    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]):
        """ä»å­—å…¸åŠ è½½é…ç½®"""
        for key, value in config_dict.items():
            if hasattr(cls, key):
                setattr(cls, key, value)

    @classmethod
    def to_dict(cls) -> Dict[str, Any]:
        """å¯¼å‡ºé…ç½®ä¸ºå­—å…¸"""
        return {
            "API_KEY": cls.API_KEY,
            "API_BASE_URL": cls.API_BASE_URL,
            "QUALITY_LEVEL": cls.QUALITY_LEVEL,
            "MAX_CONCURRENT_REQUESTS": cls.MAX_CONCURRENT_REQUESTS,
            "OUTPUT_DIR": cls.OUTPUT_DIR
        }

class EnhancedNoteToQuizGenerator(NoteToQuizGenerator):
    """ç¬”è®°ç”Ÿé¢˜å™¨"""

    def __init__(self, config: Config = None):
        if config is None:
            config = Config()

        super().__init__(config.API_KEY, config.QUALITY_LEVEL)  
        self.config = config
        self.chunker = TextChunker(quality_level=config.QUALITY_LEVEL)
        self.merger = KnowledgePointMerger(quality_level=config.QUALITY_LEVEL)
        self.reviewer = InteractiveReviewer()
        self.formatter = QuizFormatter()

    async def process_with_review(self, file_path: str, enable_review: bool = True) -> Tuple[List[KnowledgePoint], List[Question]]:
        """å¤„ç†æ–‡æ¡£å¹¶å¯é€‰åœ°è¿›è¡Œäººå·¥å®¡æ ¸"""
        text = DocumentParser.parse_document(file_path)
        chunks = self.chunker.chunk_text(text)
        raw_knowledge_points = await self.extractor.extract_all(chunks)
        merged_knowledge_points = self.merger.merge_knowledge_points(raw_knowledge_points)

        if enable_review:
            merged_knowledge_points = self.reviewer.review_knowledge_points(merged_knowledge_points)

        questions = await self.generator.generate_all(merged_knowledge_points)

        return merged_knowledge_points, questions

    def save_all_formats(self, knowledge_points: List[KnowledgePoint],
                        questions: List[Question],
                        base_name: str = "quiz"):
        """ä¿å­˜ä¸ºå¤šç§æ ¼å¼"""
        output_dir = self.config.OUTPUT_DIR
        os.makedirs(output_dir, exist_ok=True)

        # JSONæ ¼å¼
        self.save_results(knowledge_points, questions, output_dir)

        # HTMLæ ¼å¼
        html_content = self.formatter.to_html(questions, knowledge_points)
        with open(f"{output_dir}/{base_name}.html", 'w', encoding='utf-8') as f:
            f.write(html_content)

        # Markdownæ ¼å¼
        md_content = self.formatter.to_markdown(questions, knowledge_points)
        with open(f"{output_dir}/{base_name}.md", 'w', encoding='utf-8') as f:
            f.write(md_content)

        print(f"âœ… å·²ä¿å­˜ä¸ºå¤šç§æ ¼å¼åˆ° {output_dir} ç›®å½•")


class ColabHelper:
    """Colabç¯å¢ƒè¾…åŠ©å·¥å…·"""

    @staticmethod
    def setup_environment():
        """è®¾ç½®Colabç¯å¢ƒ"""
        print("ğŸ”§ æ­£åœ¨è®¾ç½®Colabç¯å¢ƒ...")

        import subprocess
        subprocess.run(["pip", "install", "-q", "PyPDF2", "python-docx",
                       "scikit-learn", "aiohttp", "nest_asyncio", "tqdm"])

        print("âœ… ä¾èµ–å®‰è£…å®Œæˆ")

    @staticmethod
    def upload_file():
        """ä¸Šä¼ æ–‡ä»¶åˆ°Colab"""
        from google.colab import files
        print("ğŸ“¤ è¯·é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡æ¡£...")
        uploaded = files.upload()

        if uploaded:
            file_name = list(uploaded.keys())[0]
            print(f"âœ… æ–‡ä»¶ '{file_name}' ä¸Šä¼ æˆåŠŸ")
            return file_name
        else:
            print("âŒ æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶")
            return None

    @staticmethod
    def download_results(output_dir: str = "output"):
        """ä¸‹è½½ç»“æœæ–‡ä»¶"""
        from google.colab import files

        print("ğŸ“¥ æ­£åœ¨å‡†å¤‡ä¸‹è½½æ–‡ä»¶...")

        import zipfile
        zip_name = "quiz_results.zip"

        with zipfile.ZipFile(zip_name, 'w') as zipf:
            for root, dirs, files_list in os.walk(output_dir):
                for file in files_list:
                    file_path = os.path.join(root, file)
                    zipf.write(file_path)

        files.download(zip_name)
        print("âœ… ç»“æœæ–‡ä»¶å·²ä¸‹è½½")

    @staticmethod
    def interactive_setup():
        """äº¤äº’å¼è®¾ç½®"""
        print("ğŸ¯ ç¬”è®°ç”Ÿé¢˜å™¨ - äº¤äº’å¼è®¾ç½®")
        print("=" * 50)

        api_key = input("è¯·è¾“å…¥ä½ çš„DeepSeek APIå¯†é’¥: ").strip()

        print("\nğŸ“‹ é€‰æ‹©é¢˜ç›®ç”Ÿæˆè´¨é‡æ¡£ä½:")
        print("1. ç®€çº¦ - åªæŠ“é‡ç‚¹çŸ¥è¯†ï¼Œç”Ÿæˆæ ¸å¿ƒé¢˜ç›®")
        print("2. ä¸­ç­‰ - å¹³è¡¡è¦†ç›–ï¼Œé€‚ä¸­çš„é¢˜ç›®æ•°é‡")
        print("3. è¾ƒç»†è‡´ - è¦†ç›–æ›´å¤šçŸ¥è¯†ç‚¹ï¼Œå¢åŠ èåˆé¢˜")
        print("4. ç»†è‡´ - è¯¦ç»†è¦†ç›–å¤§éƒ¨åˆ†çŸ¥è¯†ç‚¹ï¼Œå¤šç§é¢˜å‹")
        print("5. ç²¾ç»† - å°½å¯èƒ½æŠ“å…¨æ‰€æœ‰çŸ¥è¯†ç‚¹ï¼Œä¸°å¯Œé¢˜å‹")

        quality_choice = input("è¯·é€‰æ‹©è´¨é‡æ¡£ä½ [1-5ï¼Œé»˜è®¤2]: ").strip() or "2"

        quality_map = {
            "1": "ç®€çº¦",
            "2": "ä¸­ç­‰",
            "3": "è¾ƒç»†è‡´",
            "4": "ç»†è‡´",
            "5": "ç²¾ç»†"
        }

        quality_level = quality_map.get(quality_choice, "ä¸­ç­‰")
        print(f"å·²é€‰æ‹©è´¨é‡æ¡£ä½: {quality_level}")

        config = Config()
        config.API_KEY = api_key
        config.QUALITY_LEVEL = quality_level

        return config

async def colab_main():
    """Colabç¯å¢ƒçš„ä¸»å‡½æ•°"""
    # 1. è®¾ç½®ç¯å¢ƒ
    ColabHelper.setup_environment()

    # 2. äº¤äº’å¼é…ç½®
    config = ColabHelper.interactive_setup()

    # 3. ä¸Šä¼ æ–‡ä»¶
    file_path = ColabHelper.upload_file()
    if not file_path:
        return

    # 4. åˆ›å»ºç”Ÿæˆå™¨
    generator = EnhancedNoteToQuizGenerator(config)

    # 5. è¯¢é—®æ˜¯å¦éœ€è¦å®¡æ ¸
    enable_review = input("\næ˜¯å¦éœ€è¦äººå·¥å®¡æ ¸çŸ¥è¯†ç‚¹ï¼Ÿ[y/N]: ").lower() == 'y'

    try:
        # 6. å¤„ç†æ–‡æ¡£
        print("\nğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£...")
        start_time = time.time()

        knowledge_points, questions = await generator.process_with_review(
            file_path, enable_review
        )

        # 7. ä¿å­˜ç»“æœ
        generator.save_all_formats(knowledge_points, questions,
                                 base_name=os.path.splitext(file_path)[0])

        # 8. æ˜¾ç¤ºç»Ÿè®¡
        elapsed_time = time.time() - start_time
        print(f"\nâ±ï¸ æ€»ç”¨æ—¶: {elapsed_time:.2f} ç§’")
        print(f"ğŸ“Š ç”Ÿæˆäº† {len(questions)} é“é¢˜ç›®")

        # 9. ä¸‹è½½ç»“æœ
        if input("\næ˜¯å¦ä¸‹è½½ç»“æœæ–‡ä»¶ï¼Ÿ[Y/n]: ").lower() != 'n':
            ColabHelper.download_results(config.OUTPUT_DIR)

    except Exception as e:
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    try:
        import google.colab
        IN_COLAB = True
    except ImportError:
        IN_COLAB = False

    if IN_COLAB:
        asyncio.run(colab_main())
    else:
        asyncio.run(main())
